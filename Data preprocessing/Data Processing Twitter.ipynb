{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import PatternAnalyzer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Sentiment Analysis\n",
    "\n",
    "Before we dive into sentiment analysis, it's essential to preprocess our dataset to ensure optimal results. The following steps will guide us through this preprocessing phase:\n",
    "\n",
    "## Lowercasing\n",
    "All text will be converted to lowercase. This uniformity ensures words such as \"Ferry\", \"ferry\", and \"FERRY\" are treated identically.\n",
    "\n",
    "**Why?** \n",
    "To eliminate case-sensitivity and maintain data consistency.\n",
    "\n",
    "##  Remove Punctuation\n",
    "Punctuations can act as noise in our sentiment analysis.\n",
    "\n",
    "**Why?**\n",
    "Punctuation does not usually carry sentiment and can introduce inconsistency in the text representation.\n",
    "\n",
    "##  Remove Numbers and Special Characters\n",
    "Unless numbers carry a specific sentiment in the context of your data, they can be safely removed or replaced.\n",
    "\n",
    "**Why?**\n",
    "Numbers, in most cases, don't convey any sentiment.\n",
    "\n",
    "##  Remove Stop Words\n",
    "Words like \"and\", \"the\", \"is\", etc., are prevalent and don't typically contribute to sentiment.\n",
    "\n",
    "**Why?**\n",
    "Removing stop words helps in reducing the dimensionality of the data and focusing on words that carry sentiment. However, exercise caution as, in some instances, they might alter the sentiment's context.\n",
    "\n",
    "##  Lemmatization\n",
    "Words will be reduced to their root form using stemming.\n",
    "\n",
    "**Why?**\n",
    "By reducing words to their base form, stemming can help in reducing the dimensionality of our dataset and focusing on the core meaning of words.\n",
    "\n",
    "##  Remove URLs\n",
    "We will eliminate any URLs present in the reviews.\n",
    "\n",
    "**Why?**\n",
    "URLs are common in social media content but don't contribute to sentiment.\n",
    "\n",
    "##  Remove Usernames/Handles\n",
    "Any platform-specific usernames or handles will be removed.\n",
    "\n",
    "**Why?**\n",
    "Like URLs, usernames don't provide sentiment information and are more identifiers than content.\n",
    "\n",
    "##  Spell Correction\n",
    "We'll correct the spelling of the words in the reviews.\n",
    "\n",
    "**Why?**\n",
    "Misspellings can introduce noise. Correcting them ensures we're analyzing sentiments based on actual words. However, some misspelled words might have a unique sentiment value, so tread with caution.\n",
    "\n",
    "##  Handle Emojis\n",
    "Emojis will be converted to text or analyzed separately, as they can often carry sentiment.\n",
    "\n",
    "**Why?**\n",
    "Emojis are increasingly becoming a form of expression and can provide valuable sentiment insights.\n",
    "\n",
    "##  Remove Empty Reviews\n",
    "Rows that contain no text will be filtered out from the dataset.\n",
    "\n",
    "**Why?**\n",
    "Empty reviews can not be analysed on sentiment\n",
    "\n",
    "## Remove Duplicate Reviews\n",
    "Any repeated reviews across platforms or locations will be removed.\n",
    "\n",
    "**Why?**\n",
    "Duplicate reviews can skew the analysis by giving undue weightage to repeated sentiments.\n",
    "\n",
    "In the following cells, we will programmatically implement these preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df_original = pd.read_csv(\"twitter_output.csv\")\n",
    "df = pd.read_csv(\"Tweets_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_length = len(df_original)\n",
    "original_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove entries where caption is NaN\n",
    "df = df.dropna(subset=[\"Tweets_english\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'caption' column text to lowercase\n",
    "# df['caption'] = df['caption'].str.lower()\n",
    "df.loc[:, 'Tweets_english'] = df['Tweets_english'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from the 'caption' column\n",
    "df['Tweets_english'] = df['Tweets_english'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers and special characters from the 'caption' column\n",
    "df['Tweets_english'] = df['Tweets_english'].str.replace(r\"[0-9!&$*#]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern for URLs\n",
    "url_pattern = r'http\\S+|www\\S+'\n",
    "\n",
    "# Remove URLs from the 'caption' column\n",
    "df['Tweets_english'] = df['Tweets_english'].str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern for usernames/handles\n",
    "username_pattern = r'@\\w+'\n",
    "\n",
    "# Remove usernames from the 'caption' column\n",
    "df['Tweets_english'] = df['Tweets_english'].str.replace(username_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change emojis to their text equivalent meaning\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "df['Tweets_english'] = df['Tweets_english'].apply(convert_emojis_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'caption' is either empty or just whitespace\n",
    "df = df[df['Tweets_english'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)\n",
    "df['Tweets_english'] = df['Tweets_english'].str.replace(r'^\\s+', '', regex=True).str.replace(r'\\s+$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate df from dutch to english\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text=text)\n",
    "        time.sleep(0.2)  # sleep for 0.2 seconds\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text}. Error: {e}\")\n",
    "        return text  # return original text if translation fails\n",
    "\n",
    "tqdm.pandas()\n",
    "df['Tweets_english'] = df['Tweets'].progress_apply(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove stopwords from captions\n",
    "stop = set(stopwords.words('english')) # for English stop words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop]\n",
    "        return ' '.join(filtered_words)\n",
    "    return text  # If not a string, return the input as is\n",
    "\n",
    "df['Tweets_english'] = df['Tweets_english'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply lemmatization to the 'caption' column\n",
    "df['Tweets_english'] = df['Tweets_english'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on the 'caption' column\n",
    "df = df.drop_duplicates(subset='Tweets_english', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_length = len(df)\n",
    "print(f\"Google reviews dataset contains: {new_length} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = [original_length, new_length]\n",
    "labels = ['Total scraped', 'After Preprocessing']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=labels, y=counts, palette='viridis')\n",
    "plt.ylabel('Number of twitter posts')\n",
    "plt.title('Number of twitter posts Before and After Data Preprocessing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Ensure seaborn is imported\n",
    "\n",
    "# Tokenize words in the tweets\n",
    "words = ' '.join(df['Tweets_english'].dropna()).lower()  # Join all tweets and convert to lowercase\n",
    "tokens = word_tokenize(words)\n",
    "\n",
    "# Remove punctuation and filter stopwords\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
    "\n",
    "# Count the frequency of each word\n",
    "freq_dist = nltk.FreqDist(filtered_tokens)\n",
    "\n",
    "# Get the top 31 most common words (including the first word we want to skip)\n",
    "common_words = freq_dist.most_common(31)\n",
    "\n",
    "# Skip the first word\n",
    "common_words = common_words[1:]\n",
    "\n",
    "# Separate the words and their counts for plotting\n",
    "words, counts = zip(*common_words)\n",
    "word_count_dict = dict(zip(words, counts))\n",
    "print(word_count_dict)\n",
    "# Use a colormap to generate colors for each word\n",
    "colors = sns.color_palette(\"viridis\", len(words))\n",
    "\n",
    "# Plot the most common words with different colors\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(words, counts, color=colors)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top 30 Most Common Words in Twitter Posts (excluding ferry)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most common word on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = [TextBlob(word).sentiment.polarity for word in words]\n",
    "\n",
    "# Plot the most common words with their sentiment\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Use a colormap to generate colors based on sentiment\n",
    "colors = [sns.color_palette(\"RdBu\", 10)[int(5*(polarity + 1))] for polarity in polarities]\n",
    "\n",
    "plt.barh(words, counts, color=colors)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.title('Top 30 Most Common Words in Twitter posts with Sentiment Coloring')\n",
    "\n",
    "# Create the colorbar with the desired range\n",
    "mappable = plt.cm.ScalarMappable(cmap=\"RdBu\", norm=plt.Normalize(vmin=-1, vmax=1))\n",
    "plt.colorbar(mappable, orientation=\"vertical\", label=\"Sentiment\")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most common word on top\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
